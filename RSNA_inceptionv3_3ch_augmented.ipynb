{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "from math import ceil, floor\n",
    "import cv2\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "import sys\n",
    "\n",
    "#from keras_applications.mobilenet import MobileNet, preprocess_input\n",
    "from keras_applications.inception_v3 import InceptionV3, preprocess_input\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Input\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)  # set this TensorFlow session as the default session for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_dcm(dcm):\n",
    "    x = dcm.pixel_array + 1000\n",
    "    px_mode = 4096\n",
    "    x[x>=px_mode] = x[x>=px_mode] - px_mode\n",
    "    dcm.PixelData = x.tobytes()\n",
    "    dcm.RescaleIntercept = -1000\n",
    "\n",
    "def window_image(dcm, window_center, window_width):\n",
    "\n",
    "    if (dcm.BitsStored == 12) and (dcm.PixelRepresentation == 0) and (int(dcm.RescaleIntercept) > -100):\n",
    "        correct_dcm(dcm)\n",
    "\n",
    "    img = dcm.pixel_array * dcm.RescaleSlope + dcm.RescaleIntercept\n",
    "    img_min = window_center - window_width // 2\n",
    "    img_max = window_center + window_width // 2\n",
    "    img = np.clip(img, img_min, img_max)\n",
    "\n",
    "    return img\n",
    "\n",
    "def bsb_window(dcm):\n",
    "    brain_img = window_image(dcm, 40, 80)\n",
    "    subdural_img = window_image(dcm, 80, 200)\n",
    "    soft_img = window_image(dcm, 40, 380)\n",
    "\n",
    "    brain_img = (brain_img - 0) / 80\n",
    "    subdural_img = (subdural_img - (-20)) / 200\n",
    "    soft_img = (soft_img - (-150)) / 380\n",
    "    bsb_img = np.array([brain_img, subdural_img, soft_img]).transpose(1,2,0)\n",
    "    return bsb_img*255\n",
    "\n",
    "def process_img(img):\n",
    "    img=img.astype(np.uint8)\n",
    "    img_grsc=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    dst = cv2.GaussianBlur(img_grsc,(5,5),cv2.BORDER_DEFAULT)\n",
    "    ret, thresh = cv2.threshold(dst, 10, 255, 0)\n",
    "    contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if len(contours)>0:\n",
    "        c = max(contours, key = cv2.contourArea)\n",
    "        mask = np.zeros_like(img_grsc)\n",
    "        cv2.drawContours(mask, [c], 0, 255, -1)\n",
    "        out_img=cv2.bitwise_and(img,img,mask=mask)\n",
    "        x,y,w,h=cv2.boundingRect(c)\n",
    "        out_img=out_img[y:y+h,x:x+w]\n",
    "        out_img=cv2.resize(out_img,(512,512))\n",
    "        return out_img\n",
    "    else:\n",
    "        return img\n",
    "        \n",
    "def _read(path, desired_size=(512, 512)):\n",
    "\n",
    "    dcm = pydicom.dcmread(path)\n",
    "\n",
    "    \n",
    "\n",
    "    try:\n",
    "        # dcm.pixel_array might be corrupt (one case so far)\n",
    "        img=process_img(bsb_window(dcm))/255\n",
    "    except:\n",
    "        img = np.zeros((desired_size[0],desired_size[1],3))        \n",
    "\n",
    "    \n",
    "    if desired_size != (512, 512):\n",
    "        # resize image\n",
    "        img = cv2.resize(img, desired_size, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataGenerator(keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, list_IDs, labels, batch_size=1, img_size=(512, 512), \n",
    "                 img_dir='ds/stage_1_train_images/', *args, **kwargs):\n",
    "\n",
    "        self.list_IDs = list_IDs\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.img_dir = img_dir\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(ceil(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indices]\n",
    "        X, Y = self.__data_generation(list_IDs_temp)\n",
    "        return X, Y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indices = np.arange(len(self.list_IDs))\n",
    "        np.random.shuffle(self.indices)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        X = np.empty((self.batch_size, *self.img_size, 3))\n",
    "        Y = np.empty((self.batch_size, 6), dtype=np.float32)\n",
    "        \n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            X[i,] = _read(self.img_dir+ID+\".dcm\", self.img_size)\n",
    "            Y[i,] = self.labels.loc[ID].values\n",
    "        \n",
    "        return X, Y\n",
    "    \n",
    "    \n",
    "class TestDataGenerator(keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, list_IDs, labels, batch_size=1, img_size=(512, 512), \n",
    "                 img_dir='ds/stage_1_test_images/', *args, **kwargs):\n",
    "\n",
    "        self.list_IDs = list_IDs\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.img_dir = img_dir\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(ceil(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indices]\n",
    "        X = self.__data_generation(list_IDs_temp)\n",
    "        return X\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indices = np.arange(len(self.list_IDs))\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        X = np.empty((self.batch_size, *self.img_size, 3))\n",
    "        \n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            X[i,] = _read(self.img_dir+ID+\".dcm\", self.img_size)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import binary_crossentropy\n",
    "import keras.backend as K\n",
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = y_true_f * y_pred_f\n",
    "    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return 1. - score\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    return binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _initial_layer(input_dims):\n",
    "    inputs = keras.layers.Input(input_dims)\n",
    "    \n",
    "    x = keras.layers.Conv2D(filters=3, kernel_size=(1, 1), strides=(1, 1), name=\"initial_conv2d\")(inputs)\n",
    "    x = keras.layers.BatchNormalization(axis=3, epsilon=1.001e-5, name='initial_bn')(x)\n",
    "    x = keras.layers.Activation('relu', name='initial_relu')(x)\n",
    "    \n",
    "    return keras.models.Model(inputs, x)\n",
    "\n",
    "class MyDeepModel:\n",
    "    \n",
    "    def __init__(self, engine, input_dims, batch_size=5, learning_rate=1e-3, \n",
    "                 decay_rate=1.0, decay_steps=1, weights=\"imagenet\", verbose=1):\n",
    "        \n",
    "        self.engine = engine\n",
    "        self.input_dims = input_dims\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.decay_rate = decay_rate\n",
    "        self.decay_steps = decay_steps\n",
    "        self.weights = weights\n",
    "        self.verbose = verbose\n",
    "        self._build()\n",
    "\n",
    "    def weighted_log_loss(self,y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Can be used as the loss function in model.compile()\n",
    "        ---------------------------------------------------\n",
    "        \"\"\"\n",
    "\n",
    "        class_weights = np.array([2., 1., 1., 1., 1., 1.])\n",
    "\n",
    "        eps = K.epsilon()\n",
    "\n",
    "        y_pred = K.clip(y_pred, eps, 1.0-eps)\n",
    "\n",
    "        out = -( y_true*K.log(y_pred)*class_weights+ (1.0 - y_true) * K.log(1.0 - y_pred) * class_weights)\n",
    "\n",
    "        return K.mean(out, axis=-1)\n",
    "    def _build(self):\n",
    "        \n",
    "        engine = self.engine(include_top=False, weights=self.weights, input_shape=(*self.input_dims, 3),\n",
    "                             backend = keras.backend, layers = keras.layers,\n",
    "                             models = keras.models, utils = keras.utils)\n",
    "\n",
    "        x = engine.output\n",
    "\n",
    "        x = keras.layers.GlobalAveragePooling2D(name='avg_pool_grb')(x)\n",
    "        \n",
    "        x = keras.layers.Dropout(0.25)(x)\n",
    "\n",
    "        out = keras.layers.Dense(6, activation=\"sigmoid\", name='dense_output')(x)\n",
    "\n",
    "        self.model = keras.models.Model(inputs=engine.input, outputs=out)\n",
    "\n",
    "        self.model.compile(loss=\"binary_crossentropy\",optimizer=keras.optimizers.Adam())\n",
    "        #print(self.model.summary())\n",
    "    \n",
    "    def fit(self, df, train_idx, img_dir, global_epoch):\n",
    "        self.model.fit_generator(\n",
    "            TrainDataGenerator(\n",
    "                df.iloc[train_idx].index, \n",
    "                df.iloc[train_idx], \n",
    "                self.batch_size, \n",
    "                self.input_dims, \n",
    "                img_dir\n",
    "            ),\n",
    "            verbose=self.verbose,\n",
    "            use_multiprocessing=True,\n",
    "            workers=6,\n",
    "            callbacks=[\n",
    "                keras.callbacks.LearningRateScheduler(\n",
    "                    lambda epoch: self.learning_rate * pow(self.decay_rate, floor(global_epoch / self.decay_steps))\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    def predict(self, df, test_idx, img_dir):\n",
    "        predictions = \\\n",
    "          self.model.predict_generator(\n",
    "            TestDataGenerator(\n",
    "                df.iloc[test_idx].index, \n",
    "                None, \n",
    "                self.batch_size, \n",
    "                self.input_dims, \n",
    "                img_dir\n",
    "            ),\n",
    "            verbose=1,\n",
    "            use_multiprocessing=True,\n",
    "            workers=4\n",
    "        )\n",
    "\n",
    "        return predictions[:df.iloc[test_idx].shape[0]]\n",
    "    \n",
    "    def save(self):\n",
    "        model_json = self.model.to_json()\n",
    "        with open(\"model_mobilenet_arch.json\", \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "        self.model.save_weights(\"model_mobilenet_weights.h5\")\n",
    "    \n",
    "    def load(self, path):\n",
    "        self.model.load_weights(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_testset(filename=\"ds/stage_1_sample_submission.csv\"):\n",
    "    df = pd.read_csv(filename)\n",
    "    df[\"Image\"] = df[\"ID\"].str.slice(stop=12)\n",
    "    df[\"Diagnosis\"] = df[\"ID\"].str.slice(start=13)\n",
    "    \n",
    "    df = df.loc[:, [\"Label\", \"Diagnosis\", \"Image\"]]\n",
    "    df = df.set_index(['Image', 'Diagnosis']).unstack(level=-1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def read_trainset(filename=\"ds/stage_1_train.csv\"):\n",
    "    df = pd.read_csv(filename)\n",
    "    df[\"Image\"] = df[\"ID\"].str.slice(stop=12)\n",
    "    df[\"Diagnosis\"] = df[\"ID\"].str.slice(start=13)\n",
    "    \n",
    "    duplicates_to_remove = [\n",
    "        1598538, 1598539, 1598540, 1598541, 1598542, 1598543,\n",
    "        312468,  312469,  312470,  312471,  312472,  312473,\n",
    "        2708700, 2708701, 2708702, 2708703, 2708704, 2708705,\n",
    "        3032994, 3032995, 3032996, 3032997, 3032998, 3032999\n",
    "    ]\n",
    "    \n",
    "    df = df.drop(index=duplicates_to_remove)\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    df = df.loc[:, [\"Label\", \"Diagnosis\", \"Image\"]]\n",
    "    df = df.set_index(['Image', 'Diagnosis']).unstack(level=-1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "    \n",
    "test_df = read_testset()\n",
    "df = read_trainset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Epoch 1/1\n",
      "10879/18964 [================>.............] - ETA: 1:25:14 - loss: 0.1060"
     ]
    }
   ],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "from sklearn.metrics import log_loss\n",
    "_TEST_IMAGES = 'ds/stage_1_test_images/'\n",
    "_TRAIN_IMAGES = 'ds/stage_1_train_images/'\n",
    "\n",
    "def run(model, df, train_idx, valid_idx, test_df, epochs):\n",
    "    \n",
    "    valid_predictions = []\n",
    "    test_predictions = []\n",
    "    for global_epoch in range(epochs):\n",
    "        print(global_epoch)\n",
    "        model.fit(df, train_idx, _TRAIN_IMAGES, global_epoch)\n",
    "        \n",
    "        #if global_epoch==epochs-1:\n",
    "        test_predictions.append(model.predict(test_df, range(test_df.shape[0]), _TEST_IMAGES))\n",
    "        valid_predictions.append(model.predict(df, valid_idx, _TRAIN_IMAGES))\n",
    "        \n",
    "        print(\"validation loss: %.4f\" %\n",
    "              weighted_loss_metric(df.iloc[valid_idx].values, np.average(valid_predictions, \n",
    "                                                                         axis=0, \n",
    "                                                                         weights=[2**i for i in range(len(valid_predictions))])\n",
    "                                  )\n",
    "             )\n",
    "    model.save()\n",
    "    return test_predictions, valid_predictions\n",
    "\n",
    "# this is probably not correct, but works OK. \n",
    "#   Feel free to give feedback.\n",
    "def weighted_loss_metric(trues, preds, weights=[0.2, 0.1, 0.1, 0.1, 0.1, 0.1], clip_value=1e-7):\n",
    "    preds = np.clip(preds, clip_value, 1-clip_value)\n",
    "    loss_subtypes = trues * np.log(preds) + (1 - trues) * np.log(1 - preds)\n",
    "    loss_weighted = np.average(loss_subtypes, axis=1, weights=weights)\n",
    "    return - loss_weighted.mean()\n",
    "\n",
    "def wtlm(trues,preds):\n",
    "    return log_loss(trues,preds,sample_weight=[2,1,1,1,1,1]*trues.shape[0])\n",
    "# train set (90%) setand validation set (10%)\n",
    "ss = ShuffleSplit(n_splits=5, test_size=0.1, random_state=42).split(df.index)\n",
    "\n",
    "# will just do one fold\n",
    "train_idx, valid_idx = next(ss)\n",
    "\n",
    "# obtain model\n",
    "model = MyDeepModel(engine=InceptionV3, input_dims=(299,299), batch_size=32, learning_rate=1e-3, \n",
    "                    decay_rate=0.75, decay_steps=1, weights=\"imagenet\", verbose=1)\n",
    "\n",
    "# run 2 epochs and obtain test + validation predictions\n",
    "test_preds, _ = run(model, df, train_idx, valid_idx, test_df, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#with open(\"saved_pkl.pkl\",\"wb\") as f:\n",
    "#    pickle.dump(obj=test_preds,file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.iloc[:, :] = np.average(test_preds, axis=0, weights=[2**i for i in range(len(test_preds))])\n",
    "\n",
    "test_df = test_df.stack().reset_index()\n",
    "\n",
    "test_df.insert(loc=0, column='ID', value=test_df['Image'].astype(str) + \"_\" + test_df['Diagnosis'])\n",
    "\n",
    "test_df = test_df.drop([\"Image\", \"Diagnosis\"], axis=1)\n",
    "\n",
    "test_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('submission_three_ch_inception_v3_ep3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install vis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
